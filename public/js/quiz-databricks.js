const questions = [
    // --- QUEST√ïES ORIGINAIS (1-13) ---
    {
        question: "Qual √© a principal vantagem da arquitetura Lakehouse em rela√ß√£o a um Data Warehouse tradicional?",
        options: [
            "Suporta apenas dados estruturados.",
            "Combina a flexibilidade do Data Lake com a gest√£o e ACID do Data Warehouse.",
            "√â mais caro e complexo de manter.",
            "N√£o permite o uso de SQL."
        ],
        correctIndex: 1,
        explanation: "O Lakehouse une o melhor dos dois mundos: suporte a dados variados (como um Lake) e confiabilidade transacional (como um Warehouse)."
    },
    {
        question: "No PySpark, qual m√©todo √© usado para mostrar as primeiras linhas de um DataFrame de forma tabular?",
        options: [
            "df.print()",
            "df.view()",
            "df.show()",
            "df.display()"
        ],
        correctIndex: 2,
        explanation: "O m√©todo `show()` exibe o conte√∫do do DataFrame no console. O `display()` √© espec√≠fico de notebooks como Databricks e oferece formata√ß√£o visual rica."
    },
    {
        question: "O que o comando 'OPTIMIZE' faz em uma tabela Delta?",
        options: [
            "Apaga dados antigos.",
            "Compacta pequenos arquivos em arquivos maiores para melhorar a leitura.",
            "Cria √≠ndices autom√°ticos.",
            "Converte arquivos CSV para Parquet."
        ],
        correctIndex: 1,
        explanation: "O OPTIMIZE melhora a performance de leitura compactando pequenos arquivos (small files problem) em arquivos maiores."
    },
    {
        question: "Qual √© o principal objetivo da camada Bronze na arquitetura Medallion (Multi-hop)?",
        options: [
            "Armazenar dados agregados para relat√≥rios de BI.",
            "Manter uma c√≥pia fiel (raw) dos dados da fonte, sem transforma√ß√µes de neg√≥cio.",
            "Filtrar dados inv√°lidos e remover duplicatas.",
            "Gerenciar permiss√µes de acesso aos usu√°rios finais."
        ],
        correctIndex: 1,
        explanation: "A camada Bronze serve como um hist√≥rico inalter√°vel (Raw). Ela armazena os dados exatamente como chegaram da fonte para garantir que voc√™ sempre possa reprocessar tudo se necess√°rio."
    },
    {
        question: "Voc√™ precisa reverter uma tabela Delta para um estado anterior ap√≥s uma exclus√£o acidental. Qual funcionalidade do Delta Lake permite isso?",
        options: [
            "Schema Evolution",
            "Z-Ordering",
            "Time Travel",
            "Auto Loader"
        ],
        correctIndex: 2,
        explanation: "O Time Travel permite consultar ou restaurar vers√µes antigas da tabela usando o log de transa√ß√µes (ex: `RESTORE TABLE x TO VERSION AS OF 2`)."
    },
    {
        question: "No PySpark Structured Streaming, por que √© crucial definir um 'checkpoint location'?",
        options: [
            "Para salvar os dados finais em formato Parquet.",
            "Para garantir toler√¢ncia a falhas e continuar o processamento de onde parou em caso de erro.",
            "Para melhorar a velocidade de leitura dos dados.",
            "Para conectar com o Power BI em tempo real."
        ],
        correctIndex: 1,
        explanation: "O checkpoint armazena o progresso da stream (offsets). Se o cluster cair, o Spark l√™ o checkpoint e retoma exatamente de onde parou, garantindo consist√™ncia 'exactly-once'."
    },
    {
        question: "Qual comando SQL √© usado para converter arquivos Parquet existentes em uma tabela Delta sem reescrever os dados?",
        options: [
            "ALTER TABLE ... SET FORMAT DELTA",
            "CONVERT TO DELTA",
            "CAST(parquet AS delta)",
            "UPDATE TABLE FORMAT = 'delta'"
        ],
        correctIndex: 1,
        explanation: "O comando `CONVERT TO DELTA` cria o log de transa√ß√µes (_delta_log) para arquivos Parquet existentes, transformando-os em uma tabela Delta 'in-place'."
    },
    {
        question: "Para melhorar a performance de consultas que filtram por uma coluna espec√≠fica (ex: 'data_venda') em grandes tabelas Delta, qual t√©cnica √© recomendada?",
        options: [
            "Usar o comando VACUUM.",
            "Aumentar o tamanho do cluster.",
            "Aplicar OPTIMIZE com Z-ORDER BY na coluna.",
            "Converter a tabela para CSV."
        ],
        correctIndex: 2,
        explanation: "O `OPTIMIZE` compacta arquivos pequenos, e o `Z-ORDER` co-localiza dados relacionados nos mesmos arquivos, fazendo com que o Delta Lake pule arquivos desnecess√°rios (Data Skipping) na leitura."
    },
    {
        question: "Qual √© a estrutura de namespace de tr√™s n√≠veis utilizada pelo Unity Catalog?",
        options: [
            "schema.database.table",
            "catalog.schema.table",
            "container.folder.file",
            "project.dataset.table"
        ],
        correctIndex: 1,
        explanation: "No Unity Catalog, a hierarquia √©: Cat√°logo (n√≠vel mais alto) > Schema (ou Database) > Tabela/View."
    },
    {
        question: "O que acontece se voc√™ executar o comando VACUUM em uma tabela Delta com a reten√ß√£o padr√£o?",
        options: [
            "Ele apaga todos os dados da tabela.",
            "Ele remove arquivos f√≠sicos que n√£o s√£o mais referenciados no log h√° mais de 7 dias.",
            "Ele otimiza os √≠ndices da tabela.",
            "Ele arquiva os dados para armazenamento frio (Glacier)."
        ],
        correctIndex: 1,
        explanation: "O VACUUM limpa o armazenamento removendo arquivos de dados antigos que n√£o fazem mais parte da vers√£o atual da tabela, impedindo o Time Travel para vers√µes anteriores ao per√≠odo de reten√ß√£o."
    },
    {
        question: "Qual a fun√ß√£o do Auto Loader (cloud_files) no Databricks?",
        options: [
            "Carregar dados automaticamente para o Power BI.",
            "Ingerir arquivos novos de forma incremental e eficiente de um Data Lake (S3, ADLS) conforme eles chegam.",
            "Criar clusters automaticamente quando o usu√°rio faz login.",
            "Gerar c√≥digo SQL autom√°tico."
        ],
        correctIndex: 1,
        explanation: "O Auto Loader √© projetado para processar novos arquivos de dados conforme eles chegam no armazenamento em nuvem, gerenciando o estado e evitando a necessidade de listar todos os arquivos repetidamente."
    },
    {
        question: "Em um Delta Live Tables (DLT), qual a diferen√ßa entre uma 'Streaming Table' e uma 'Materialized View' (ou Live Table)?",
        options: [
            "N√£o h√° diferen√ßa.",
            "Streaming Tables processam cada registro apenas uma vez (incremental), enquanto Materialized Views recalculam tudo do zero a cada atualiza√ß√£o.",
            "Streaming Tables s√£o mais lentas.",
            "Materialized Views s√≥ funcionam com SQL."
        ],
        correctIndex: 1,
        explanation: "Streaming Tables s√£o ideais para ingest√£o cont√≠nua (append-only). Materialized Views (Live Tables) s√£o melhores para agrega√ß√µes onde o resultado muda conforme os dados subjacentes mudam."
    },
    {
        question: "Qual √© a diferen√ßa entre uma Managed Table e uma External Table no Databricks?",
        options: [
            "Tabelas gerenciadas s√£o mais r√°pidas.",
            "Se voc√™ dropar (DROP) uma Managed Table, os dados e metadados s√£o apagados. Na External, apenas os metadados s√£o apagados.",
            "Tabelas externas n√£o suportam Delta Lake.",
            "Tabelas gerenciadas ficam salvas no seu computador local."
        ],
        correctIndex: 1,
        explanation: "Managed Tables: O Databricks gerencia o ciclo de vida dos arquivos. External Tables: Voc√™ gerencia os arquivos; o Databricks s√≥ gerencia o ponteiro (metadados) para eles."
    },

    // --- NOVAS QUEST√ïES (14-50) ---

    // SECTION 1: DATABRICKS INTELLIGENCE PLATFORM
    {
        question: "Qual recurso do Databricks utiliza um motor de execu√ß√£o vetorizado escrito em C++ para acelerar consultas Spark?",
        options: [
            "Databricks Jobs",
            "Photon Engine",
            "Unity Catalog",
            "MLflow"
        ],
        correctIndex: 1,
        explanation: "O Photon √© o motor de execu√ß√£o nativo de alta performance do Databricks, projetado para acelerar cargas de trabalho SQL e DataFrame API."
    },
    {
        question: "Qual tipo de compute √© recomendado para executar tarefas de orquestra√ß√£o agendadas e automatizadas para economizar custos?",
        options: [
            "All-Purpose Compute",
            "Job Compute",
            "Personal Compute",
            "GPU Optimized Compute"
        ],
        correctIndex: 1,
        explanation: "O Job Compute (Cluster de Jobs) √© criado apenas para a execu√ß√£o da tarefa e encerrado ao final, sendo muito mais barato que um All-Purpose Compute interativo."
    },
    {
        question: "O que caracteriza o Databricks Serverless SQL Warehouse?",
        options: [
            "O usu√°rio precisa gerenciar manualmente as vers√µes do Spark.",
            "N√£o h√° tempo de espera para inicializa√ß√£o do cluster, pois a computa√ß√£o √© gerenciada pelo Databricks.",
            "Ele s√≥ suporta Python e Scala.",
            "Ele armazena dados diretamente na mem√≥ria RAM do usu√°rio."
        ],
        correctIndex: 1,
        explanation: "O Serverless SQL Warehouse elimina a necessidade de gerenciar infraestrutura e inicia quase instantaneamente."
    },

    // SECTION 2: DEVELOPMENT AND INGESTION
    {
        question: "Qual ferramenta permite desenvolver c√≥digo localmente na sua IDE favorita (VSCode, PyCharm) e execut√°-lo no cluster Databricks?",
        options: [
            "Databricks SQL",
            "Databricks CLI",
            "Databricks Connect",
            "Auto Loader"
        ],
        correctIndex: 2,
        explanation: "O Databricks Connect permite conectar IDEs locais aos clusters Databricks para desenvolvimento e testes remotos."
    },
    {
        question: "Ao usar o Auto Loader, qual op√ß√£o deve ser habilitada para garantir que colunas novas ou inesperadas n√£o quebrem o pipeline e sejam salvas separadamente?",
        options: [
            "mergeSchema",
            "failOnUnknown",
            "schemaHints",
            "cloudFiles.schemaEvolutionMode = 'rescue'"
        ],
        correctIndex: 3,
        explanation: "O modo de evolu√ß√£o de esquema com a coluna de dados resgatados (_rescued_data) garante que dados com tipos incorretos ou colunas novas sejam salvos sem falhar o job."
    },
    {
        question: "Qual sintaxe √© correta para ler arquivos JSON usando Auto Loader em PySpark?",
        options: [
            "spark.read.json('path')",
            "spark.readStream.format('cloudFiles').option('cloudFiles.format', 'json').load('path')",
            "spark.read.format('autoloader').load('path')",
            "spark.readStream.json('path')"
        ],
        correctIndex: 1,
        explanation: "O Auto Loader √© invocado pelo formato 'cloudFiles' e exige que o formato original do arquivo seja passado na op√ß√£o 'cloudFiles.format'."
    },
    {
        question: "Se voc√™ precisa clonar um reposit√≥rio Git dentro do Databricks para versionar seus notebooks, qual recurso voc√™ usa?",
        options: [
            "DBFS",
            "Databricks Repos (Git Folders)",
            "Unity Catalog Volumes",
            "System Tables"
        ],
        correctIndex: 1,
        explanation: "Os Databricks Repos (agora chamados Git Folders) permitem sincroniza√ß√£o direta com provedores Git como GitHub, GitLab e Azure DevOps."
    },
    
    // SECTION 3: DATA PROCESSING & TRANSFORMATIONS
    {
        question: "Qual √© o objetivo principal da camada Silver na arquitetura Medallion?",
        options: [
            "Ingest√£o bruta.",
            "Agrega√ß√µes finais para dashboards.",
            "Limpeza, deduplica√ß√£o e enriquecimento dos dados.",
            "Arquivamento de dados hist√≥ricos."
        ],
        correctIndex: 2,
        explanation: "A camada Silver √© onde os dados s√£o limpos, tipados corretamente, filtrados e enriquecidos (joins), servindo como uma 'single source of truth'."
    },
    {
        question: "Qual comando PySpark √© usado para remover linhas duplicadas de um DataFrame?",
        options: [
            "df.unique()",
            "df.dropDuplicates()",
            "df.distinct_rows()",
            "df.clean()"
        ],
        correctIndex: 1,
        explanation: "`dropDuplicates()` (ou `distinct()`) √© o m√©todo padr√£o para remover duplicatas. Voc√™ pode passar colunas espec√≠ficas como argumento."
    },
    {
        question: "Como voc√™ pode transformar uma coluna que cont√©m um array de valores em m√∫ltiplas linhas (uma para cada elemento do array)?",
        options: [
            "split()",
            "explode()",
            "flatten()",
            "array_expand()"
        ],
        correctIndex: 1,
        explanation: "A fun√ß√£o `explode()` pega uma coluna de array e cria uma nova linha para cada elemento desse array, repetindo os valores das outras colunas."
    },
    {
        question: "O que acontece se voc√™ usar 'mode(\"overwrite\")' ao salvar um DataFrame como tabela Delta?",
        options: [
            "Ele falha se a tabela j√° existir.",
            "Ele adiciona os novos dados ao final da tabela.",
            "Ele substitui atomicamente todos os dados da tabela pelos novos dados.",
            "Ele deleta a tabela e o schema."
        ],
        correctIndex: 2,
        explanation: "O modo overwrite substitui os dados existentes. No Delta Lake, isso √© feito de forma transacional e segura, mantendo o hist√≥rico para Time Travel."
    },
    {
        question: "Em um pipeline Delta Live Tables (DLT), como voc√™ define uma regra de qualidade que alerta mas n√£o interrompe o processamento?",
        options: [
            "@dlt.expect_or_fail",
            "@dlt.expect_or_drop",
            "@dlt.expect",
            "@dlt.validate"
        ],
        correctIndex: 2,
        explanation: "`@dlt.expect` registra a viola√ß√£o da regra nos logs de eventos, mas permite que o registro seja processado. `expect_or_fail` para o pipeline, e `expect_or_drop` remove o registro."
    },
    {
        question: "Qual tipo de JOIN √© mais eficiente quando uma das tabelas √© muito pequena (ex: tabela de dimens√£o)?",
        options: [
            "Sort Merge Join",
            "Shuffle Hash Join",
            "Broadcast Join",
            "Cartesian Join"
        ],
        correctIndex: 2,
        explanation: "O Broadcast Join envia a tabela pequena para todos os n√≥s do cluster, evitando o tr√°fego de rede (shuffle) da tabela grande."
    },
    
    // SECTION 4: PRODUCTIONIZING DATA PIPELINES
    {
        question: "Qual √© a ferramenta de Infraestrutura como C√≥digo (IaC) nativa do Databricks para gerenciar projetos completos (jobs, pipelines, arquivos)?",
        options: [
            "Terraform Provider",
            "Databricks Asset Bundles (DABs)",
            "CloudFormation",
            "DBFS CLI"
        ],
        correctIndex: 1,
        explanation: "DABs permitem definir e implantar projetos complexos de dados e ML usando arquivos YAML, facilitando CI/CD e boas pr√°ticas de engenharia de software."
    },
    {
        question: "Qual arquivo √© usado para configurar um Databricks Asset Bundle?",
        options: [
            "pipeline.json",
            "databricks.yml",
            "config.xml",
            "bundle.yaml"
        ],
        correctIndex: 1,
        explanation: "O arquivo `databricks.yml` na raiz do projeto define os recursos, alvos (dev, prod) e vari√°veis do bundle."
    },
    {
        question: "Se uma tarefa em um Job com m√∫ltiplas tarefas falhar, qual recurso permite reexecutar apenas a tarefa falha e suas dependentes?",
        options: [
            "Full Retry",
            "Repair and Rerun",
            "Time Travel",
            "Job Cloning"
        ],
        correctIndex: 1,
        explanation: "O recurso 'Repair and Rerun' permite selecionar o run falho e reexecutar apenas as partes que n√£o tiveram sucesso, economizando tempo e custo."
    },
    {
        question: "Onde voc√™ deve procurar para analisar o plano de execu√ß√£o f√≠sico de uma query Spark lenta?",
        options: [
            "Driver Logs",
            "Spark UI",
            "Databricks CLI",
            "Unity Catalog Lineage"
        ],
        correctIndex: 1,
        explanation: "A Spark UI fornece detalhes visuais sobre Jobs, Stages e Tasks, incluindo DAGs de execu√ß√£o e m√©tricas de shuffle/spill."
    },

    // SECTION 5: DATA GOVERNANCE & QUALITY
    {
        question: "Qual comando SQL √© usado para dar permiss√£o de leitura em uma tabela no Unity Catalog?",
        options: [
            "ALLOW READ ON TABLE",
            "GRANT SELECT ON TABLE",
            "PERMIT VIEW ON TABLE",
            "GIVE ACCESS TO"
        ],
        correctIndex: 1,
        explanation: "O padr√£o ANSI SQL `GRANT SELECT ON TABLE <tabela> TO <usuario/grupo>` √© usado no Unity Catalog para controle de acesso."
    },
    {
        question: "No Unity Catalog, o que √© um 'External Location'?",
        options: [
            "Um local f√≠sico onde o Databricks est√° instalado.",
            "Um objeto que combina uma credencial de armazenamento com um caminho na nuvem (S3/ADLS) para permitir acesso seguro.",
            "Uma tabela que foi deletada.",
            "Uma regi√£o da AWS/Azure."
        ],
        correctIndex: 1,
        explanation: "External Locations governam o acesso a caminhos de armazenamento em nuvem, permitindo a cria√ß√£o de Tabelas Externas e Volumes sem expor chaves de acesso diretamente."
    },
    {
        question: "Qual recurso do Unity Catalog permite rastrear a origem e o destino dos dados (colunas e tabelas) atrav√©s dos pipelines?",
        options: [
            "Data Lineage",
            "Audit Logs",
            "Delta Sharing",
            "System Tables"
        ],
        correctIndex: 0,
        explanation: "Data Lineage captura automaticamente as depend√™ncias entre tabelas, views e colunas conforme as queries s√£o executadas."
    },
    {
        question: "O Delta Sharing permite compartilhar dados com:",
        options: [
            "Apenas usu√°rios na mesma conta Databricks.",
            "Apenas usu√°rios usando Databricks em outras contas.",
            "Qualquer plataforma de computa√ß√£o que tenha um conector Delta Sharing (PowerBI, Pandas, Spark, etc).",
            "Apenas via email com arquivos CSV."
        ],
        correctIndex: 2,
        explanation: "O Delta Sharing √© um protocolo aberto. O consumidor n√£o precisa usar Databricks; basta ter um cliente compat√≠vel para ler os dados."
    },
    {
        question: "O que √© o Lakehouse Federation?",
        options: [
            "Uma forma de unir v√°rios workspaces Databricks.",
            "A capacidade de consultar dados em sistemas externos (PostgreSQL, Snowflake, SQL Server) sem precisar copi√°-los para o Databricks.",
            "Uma ferramenta de visualiza√ß√£o.",
            "O novo nome do Delta Lake."
        ],
        correctIndex: 1,
        explanation: "Lakehouse Federation permite criar conex√µes e cat√°logos estrangeiros para executar queries federadas em dados que residem fora do Databricks."
    },
    {
        question: "Onde ficam armazenados os logs de auditoria (quem acessou o qu√™) no Databricks com Unity Catalog habilitado?",
        options: [
            "Em arquivos de texto no driver.",
            "Nas System Tables (tabelas do sistema).",
            "N√£o s√£o armazenados.",
            "Apenas no portal da AWS/Azure."
        ],
        correctIndex: 1,
        explanation: "As System Tables (dentro do cat√°logo `system`) centralizam logs de auditoria, faturamento e linhagem acess√≠veis via SQL."
    },
    
    // EXTRAS / MIXED TOPICS TO REACH 50
    {
        question: "Qual a diferen√ßa entre 'Deep Clone' e 'Shallow Clone' no Delta Lake?",
        options: [
            "Shallow copia dados, Deep copia metadados.",
            "Deep copia dados e metadados; Shallow copia apenas metadados (ponteiros).",
            "N√£o existe diferen√ßa.",
            "Shallow √© mais caro que Deep."
        ],
        correctIndex: 1,
        explanation: "Shallow Clone √© r√°pido e barato pois n√£o duplica os arquivos f√≠sicos (√≥timo para testes). Deep Clone duplica tudo, criando uma c√≥pia independente."
    },
    {
        question: "Qual fun√ß√£o de janela (Window Function) √© usada para obter o valor da linha anterior em um conjunto de dados ordenado?",
        options: [
            "lead()",
            "lag()",
            "rank()",
            "previous()"
        ],
        correctIndex: 1,
        explanation: "`lag(coluna, 1)` pega o valor da linha anterior. `lead()` pega o valor da linha seguinte."
    },
    {
        question: "Como voc√™ cria uma Visualiza√ß√£o Materializada (Materialized View) usando SQL no Databricks?",
        options: [
            "CREATE TABLE AS SELECT...",
            "CREATE MATERIALIZED VIEW AS SELECT...",
            "CREATE LIVE TABLE ... AS SELECT...",
            "CREATE VIEW ... AS SELECT..."
        ],
        correctIndex: 1,
        explanation: "Em SQL Warehouses ou DLT, usa-se `CREATE MATERIALIZED VIEW`. Ela pr√©-computa e armazena os resultados, atualizando-os incrementalmente."
    },
    {
        question: "Qual √© o comportamento padr√£o de uma tarefa de Job se o cluster falhar?",
        options: [
            "A tarefa √© marcada como sucesso.",
            "O Job tenta reiniciar indefinidamente.",
            "A tarefa falha, a menos que uma pol√≠tica de repeti√ß√£o (Retry Policy) esteja configurada.",
            "O Job envia um email e pausa."
        ],
        correctIndex: 2,
        explanation: "Por padr√£o, a falha encerra a tarefa. Configurar 'Retries' √© uma boa pr√°tica para erros transientes."
    },
    {
        question: "No contexto de Unity Catalog, o que √© um 'Volume'?",
        options: [
            "Uma parti√ß√£o de disco.",
            "Um objeto para governar arquivos n√£o-tabulares (imagens, PDFs, CSVs brutos) dentro do cat√°logo.",
            "A quantidade de dados processados.",
            "Um tipo de cluster."
        ],
        correctIndex: 1,
        explanation: "Volumes permitem gerenciar, governar e acessar dados n√£o estruturados da mesma forma que tabelas, usando caminhos como `/Volumes/catalogo/schema/volume/arquivo.txt`."
    },
    {
        question: "Qual comando Python √© usado para instalar uma biblioteca de terceiros (ex: pandas) em um notebook com escopo de sess√£o?",
        options: [
            "install pandas",
            "%pip install pandas",
            "pip.get('pandas')",
            "dbutils.library.install('pandas')"
        ],
        correctIndex: 1,
        explanation: "O comando m√°gico `%pip install` instala bibliotecas apenas para a sess√£o atual do notebook, sem afetar outros usu√°rios do cluster."
    },
    {
        question: "Qual √© a vantagem de usar colunas geradas (Generated Columns) em tabelas Delta?",
        options: [
            "Elas criptografam os dados.",
            "Elas geram valores automaticamente baseados em outras colunas e permitem Partition Pruning autom√°tico.",
            "Elas aumentam o tamanho do arquivo.",
            "Elas s√≥ funcionam com datas."
        ],
        correctIndex: 1,
        explanation: "Exemplo: Gerar uma coluna 'data' a partir de 'timestamp'. Se voc√™ filtrar por 'timestamp', o Delta automaticamente usa a parti√ß√£o 'data' para pular arquivos (pruning)."
    },
    {
        question: "Para o que serve a instru√ß√£o 'CACHE TABLE'?",
        options: [
            "Salvar a tabela no disco permanentemente.",
            "Armazenar a tabela na mem√≥ria do cluster para acelerar consultas subsequentes.",
            "Limpar o cache do navegador.",
            "Fazer backup da tabela."
        ],
        correctIndex: 1,
        explanation: "O cache mant√©m os dados quentes na mem√≥ria dos workers. √ötil para tabelas pequenas ou m√©dias acessadas repetidamente durante uma sess√£o."
    },
    {
        question: "No Delta Lake, o que √© o 'Change Data Feed' (CDF)?",
        options: [
            "Um feed RSS de not√≠cias.",
            "Um recurso que registra mudan√ßas em n√≠vel de linha (inserts, updates, deletes) para facilitar o consumo downstream.",
            "Uma forma de mudar o tipo de dados.",
            "Um log de erros."
        ],
        correctIndex: 1,
        explanation: "CDF permite ler apenas o que mudou (delta) entre vers√µes, facilitando a propaga√ß√£o de atualiza√ß√µes e dele√ß√µes para tabelas Silver/Gold."
    },
    {
        question: "Qual √© a principal fun√ß√£o do Databricks SQL Dashboard?",
        options: [
            "Escrever c√≥digo Python.",
            "Criar visualiza√ß√µes e pain√©is interativos para usu√°rios de neg√≥cios baseados em queries SQL.",
            "Gerenciar clusters.",
            "Configurar Git."
        ],
        correctIndex: 1,
        explanation: "Databricks SQL √© focado em analistas de BI, permitindo criar dashboards ricos e alertas SQL sem usar notebooks."
    },
    {
        question: "Ao ler muitos arquivos pequenos de uma vez, qual problema de performance pode ocorrer e como o Databricks resolve na ingest√£o?",
        options: [
            "Problema de listagem de arquivos. O Auto Loader resolve com o modo de notifica√ß√£o de arquivo ou listagem incremental.",
            "O cluster desliga. Resolve com Serverless.",
            "Os dados ficam corrompidos. Resolve com Delta Checksum.",
            "Nenhum problema ocorre."
        ],
        correctIndex: 0,
        explanation: "Listar milh√µes de arquivos no S3/ADLS √© lento. O Auto Loader otimiza isso usando filas de notifica√ß√£o (SQS/Event Grid) ou listagem incremental inteligente."
    },
    {
        question: "O que a cl√°usula 'CONSTRAINT' faz em uma tabela Delta?",
        options: [
            "Define chaves estrangeiras (FK) que s√£o for√ßadas fisicamente.",
            "Define regras de valida√ß√£o (como CHECK constraints) que impedem a inser√ß√£o de dados inv√°lidos.",
            "Apenas documenta a tabela.",
            "Cria um √≠ndice secund√°rio."
        ],
        correctIndex: 1,
        explanation: "Delta Lake suporta `NOT NULL` e `CHECK` constraints. Se um dado violar a regra, a transa√ß√£o falha e o erro √© retornado."
    },
    {
        question: "Qual a melhor pr√°tica para configurar o tamanho do cluster para um Job de produ√ß√£o est√°vel?",
        options: [
            "Usar Autoscaling com limites muito distantes (min 1, max 100).",
            "Usar um tamanho fixo de cluster dimensionado para a carga de trabalho, para evitar overhead de scaling.",
            "Sempre usar o maior driver poss√≠vel.",
            "Usar Single Node."
        ],
        correctIndex: 1,
        explanation: "Para Jobs previs√≠veis, um cluster de tamanho fixo costuma ser mais perform√°tico pois n√£o perde tempo decidindo quando subir ou descer n√≥s."
    },
    {
        question: "Como voc√™ referencia um par√¢metro din√¢mico (widget) dentro de uma c√©lula SQL em um notebook?",
        options: [
            "$parametro",
            ":parametro",
            "{{parametro}}",
            "@parametro"
        ],
        correctIndex: 0,
        explanation: "Em notebooks Databricks antigos usava-se Widgets. A sintaxe SQL para pegar o valor √© nomear o widget e usar `$nome_do_widget` (embora par√¢metros de query Marker `?` ou `:name` sejam modernos)."
    },
    {
        question: "O que √© um 'Job Task' do tipo 'Run Job'?",
        options: [
            "Uma tarefa que roda um script Python.",
            "Uma tarefa que aciona outro Job Databricks, permitindo modularizar workflows.",
            "Uma tarefa que corre uma maratona.",
            "Um erro de configura√ß√£o."
        ],
        correctIndex: 1,
        explanation: "Isso permite criar pipelines complexos onde um 'Job Controlador' chama v√°rios 'Jobs Filhos' reutiliz√°veis."
    },
    {
        question: "Qual a fun√ß√£o da instru√ß√£o `MERGE INTO`?",
        options: [
            "Unir dois DataFrames apenas para leitura.",
            "Atualizar, inserir ou deletar linhas em uma tabela Delta com base em uma condi√ß√£o de match com uma tabela fonte (Upsert).",
            "Criar uma tabela nova.",
            "Mesclar c√©lulas do notebook."
        ],
        correctIndex: 1,
        explanation: "√â o comando padr√£o para opera√ß√µes de UPSERT (Update + Insert) e deduplica√ß√£o em Data Warehousing."
    },
    {
        question: "Qual privil√©gio no Unity Catalog √© necess√°rio para criar um Cat√°logo novo?",
        options: [
            "CREATE CATALOG no Metastore.",
            "CREATE SCHEMA.",
            "SELECT ANY FILE.",
            "USAGE."
        ],
        correctIndex: 0,
        explanation: "Apenas administradores de metastore ou usu√°rios com privil√©gio `CREATE CATALOG` no metastore podem criar novos cat√°logos de topo."
    }
];

let currentQuestionIndex = 0;
let score = 0;

document.addEventListener("DOMContentLoaded", () => {
    loadQuestion();
});

function loadQuestion() {
    const questionEl = document.getElementById('question');
    const optionsEl = document.getElementById('options-container');
    const nextBtn = document.getElementById('next-btn');
    const explanationDiv = document.getElementById('explanation');
    const currentQSpan = document.getElementById('current-q');
    const totalQSpan = document.getElementById('total-q');

    // Reset
    nextBtn.style.display = 'none';
    explanationDiv.style.display = 'none';
    optionsEl.innerHTML = '';

    const currentData = questions[currentQuestionIndex];

    // Textos
    questionEl.textContent = currentData.question;
    currentQSpan.textContent = currentQuestionIndex + 1;
    totalQSpan.textContent = questions.length;

    // Bot√µes
    currentData.options.forEach((opt, index) => {
        const btn = document.createElement('button');
        btn.classList.add('option-btn');
        btn.textContent = opt;
        btn.onclick = () => checkAnswer(index, btn);
        optionsEl.appendChild(btn);
    });
}

function checkAnswer(selectedIndex, btnElement) {
    const currentData = questions[currentQuestionIndex];
    const buttons = document.querySelectorAll('.option-btn');
    const explanationText = document.getElementById('explanation-text');

    buttons.forEach(btn => btn.disabled = true);

    if (selectedIndex === currentData.correctIndex) {
        score++;
        btnElement.classList.add('correct');
        explanationText.innerHTML = "<strong>Correto! üéâ</strong> " + currentData.explanation;
    } else {
        btnElement.classList.add('wrong');
        buttons[currentData.correctIndex].classList.add('correct');
        explanationText.innerHTML = "<strong>Ops! üòÖ</strong> " + currentData.explanation;
    }

    document.getElementById('explanation').style.display = 'block';
    document.getElementById('next-btn').style.display = 'block';
}

function nextQuestion() {
    currentQuestionIndex++;
    if (currentQuestionIndex < questions.length) {
        loadQuestion();
    } else {
        showResults();
    }
}

function showResults() {
    document.getElementById('quiz-screen').style.display = 'none';
    document.getElementById('result-screen').style.display = 'block';
    document.getElementById('final-score').textContent = score;
    document.getElementById('final-total').textContent = questions.length;
}

function restartQuiz() {
    currentQuestionIndex = 0;
    score = 0;
    document.getElementById('quiz-screen').style.display = 'block';
    document.getElementById('result-screen').style.display = 'none';
    loadQuestion();
}